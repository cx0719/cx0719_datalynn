{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1717192163147,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "IndfaeZbe930",
    "outputId": "4c62f726-948e-434a-c418-f4a0e81b6f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up local environment and load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1717192163147,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "K77YMd7NS7Yw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "download_path = os.path.expanduser(\"~/Downloads/so_affiliate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1717192163544,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "711p4BMOfD8k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiayuanpeng/Downloads/so_affiliate/master_pool\n"
     ]
    }
   ],
   "source": [
    "# set the path for this notebook to run on local environment.    t\n",
    "base_path =f'{download_path}/master_pool'\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1717192163783,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "o6hPYvZ6fMWs",
    "outputId": "a5cb3f53-5bf0-4a37-faf2-18c932561bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echotik_upload:  []\n",
      "manual_upload:  ['Elaiza_updateinfo_20240601.xlsx', 'Elaiza_updaterate_20240531.xlsx', 'Fhaye_scouting_MorpheJbDolceByrd_20240603.xlsx']\n",
      "master_pool_view:  ['affiliate_masterpool.csv']\n",
      "clean_view:  ['clean_view.xlsx']\n",
      "blacklist:  ['blacklist.xlsx']\n",
      "upload_error_check:  ['2024-05-03 14_54_29_Elaiza_updateproductinfo_tower28_20240427_notthandles.xlsx', '2024-04-05 11_52_20_Elaiza_scoutingbaborpacas_20240405_no_uploaddate.xlsx', '2024-05-03 14_54_29_Elaiza_updateproductinfo_milk_20240427_notthandles.xlsx', '2024-05-03 14_58_48_Elaiza_updateproductinfo_tower28_20240427_notthandles.xlsx', '2024-05-31 14_32_06_emails_20240531_wrongcolumn_Unnamed_ 2, Unnamed_ 3, Unnamed_ 4.xlsx', '2024-05-15 17_37_17_Fhaye_scouting_jb_20240509_notthandles.xlsx', '2024-05-31 14_32_06_Elaiza_updaterate_20240531_notthandles.xlsx', '2024-05-29 11_57_09_Fhaye_scouting_DolceJbNhie_20240520_no_uploaddate.xlsx', '2024-03-25 15_22_00_Fhaye_renewemail_20240322_wrongcolumn_product_name, approved_date, delivered_date.xlsx', '2024-05-03 14_58_48_Elaiza_updateproductinfo_milk_20240427_notthandles.xlsx', '2024-04-02 12_52_04_Elaiza_scoutingaccount_20240330_wrongcolumn_followers.xlsx', '2024-04-05 15_06_08_徐杭 358 100_wrongcolumn_Unnamed_ 0, tt_commerce.xlsx', '2024-05-10 15_23_53_Fhaye_scouting_jb_20240509_wrongcolumn_emails.xlsx', '2024-05-31 14_49_25_Elaiza_updaterate_20240531_notthandles.xlsx', '2024-05-31 14_32_06_Fhaye_scouting_DolceJbNhie_20240520_no_uploaddate.xlsx', '2024-04-05 11_52_20_Elaiza_scoutingaccount_20240403_notthandles.xlsx', '2024-05-29 11_57_09_Isabelle_scouting_ptr_20240523_no_uploaddate.xlsx', '2024-05-15 17_47_46_Fhaye_scouting_jb_20240509_notthandles.xlsx', '2024-04-11 15_42_27_Elaiza_scoutingaccount_20240403_notthandles.xlsx', '2024-05-10 15_42_19_Fhaye_scouting_jb_20240509_wrongcolumn_emails.xlsx', '2024-05-15 17_44_34_Fhaye_scouting_jb_20240509_notthandles.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# Base path for master_pool\n",
    "base_path =f'{download_path}/master_pool'\n",
    "\n",
    "# Path for echotick upload\n",
    "echotik_folder_path = f'{base_path}/echotik_upload'\n",
    "# list all files in the echotik_upload folder by using list comprehension\n",
    "echotik_file_names = [f for f in os.listdir(echotik_folder_path) if os.path.isfile(os.path.join(echotik_folder_path, f))]\n",
    "# print all file names found in the echotik_upload directory\n",
    "print('echotik_upload: ', echotik_file_names)\n",
    "\n",
    "# Path for manual upload\n",
    "manual_folder_path = f'{base_path}/manual_upload'\n",
    "# list all files in the manual_upload folder by using list comprehension\n",
    "manual_file_names = [f for f in os.listdir(manual_folder_path) if os.path.isfile(os.path.join(manual_folder_path, f))]\n",
    "# print all file names found in the manual_upload directory\n",
    "print('manual_upload: ', manual_file_names)\n",
    "\n",
    "# Path for master pool view\n",
    "master_pool_path = f'{base_path}/master_pool_view'\n",
    "# list all files in the master_pool_view folder by using list comprehension\n",
    "main_file_names = [f for f in os.listdir(master_pool_path) if os.path.isfile(os.path.join(master_pool_path, f))]\n",
    "# print all file names found in the master_pool_view directory\n",
    "print('master_pool_view: ', main_file_names)\n",
    "\n",
    "# Path for clean view\n",
    "clean_view_path = f'{base_path}/clean_view'\n",
    "# list all files in the clean_view folder by using list comprehension\n",
    "cleanview_file_names = [f for f in os.listdir(clean_view_path) if os.path.isfile(os.path.join(clean_view_path, f))]\n",
    "# print all file names found in the clean_view directory\n",
    "print('clean_view: ', cleanview_file_names)\n",
    "\n",
    "# Path for the blacklist\n",
    "delete_creator_path = f'{base_path}/blacklist'\n",
    "# list all files in the blacklist folder by using list comprehension\n",
    "deletecreator_file_names = [f for f in os.listdir(delete_creator_path) if os.path.isfile(os.path.join(delete_creator_path, f))]\n",
    "# print all file names found in the blacklist directory\n",
    "print('blacklist: ', deletecreator_file_names)\n",
    "\n",
    "# Path for upload_error_check\n",
    "error_check_path = f'{base_path}/upload_error_check'\n",
    "# list all files in the upload_error_check folder by using list comprehension\n",
    "errorcheck_file_names = [f for f in os.listdir(error_check_path) if os.path.isfile(os.path.join(error_check_path, f))]\n",
    "# print all file names found in the upload_error_check directory\n",
    "print('upload_error_check: ', errorcheck_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5398,
     "status": "ok",
     "timestamp": 1717192169179,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "gRU0XO90a7Ch",
    "outputId": "f6fa1a9e-cd77-48fc-e6b8-e1d6bf03e0b8"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (438741831.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [8]\u001b[0;36m\u001b[0m\n\u001b[0;31m    .str.replace('\\n', '', regex=True) \\  # # remove the new line characters\u001b[0m\n\u001b[0m                                                                            \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Read the master pool CSV file into a DataFrame\n",
    "# its only returns the 1st file within the master_pool folder\n",
    "# dtype=str ensures that all data is read to avoid datatype mismatches\n",
    "master = pd.read_csv(f'{master_pool_path}/{main_file_names[0]}', dtype=str)\n",
    "\n",
    "# clean the tt_handles column in the master dataframe\n",
    "# remove the new line characters\n",
    "master['tt_handles'] = master['tt_handles'].str.replace('\\n', '', regex=True)\n",
    "# remove the carriage return characters preventing overwritten contents\n",
    "master['tt_handles'] = master['tt_handles'].str.replace('\\r', '', regex=True)\n",
    "# replace multiple whitespaces to a single whitespace\n",
    "master['tt_handles'] = master['tt_handles'].str.replace('\\s+', ' ', regex=True)\n",
    "# replace one or more non-breaking space characters with a white space, allow us to split across lines if needed\n",
    "master['tt_handles'] = master['tt_handles'].str.replace('\\xa0+', ' ', regex=True)\n",
    "# remove leading and trailing whitespace\n",
    "master['tt_handles'] = master['tt_handles'].str.strip()\n",
    "\n",
    "# get all column names for the master dataframe\n",
    "master_column = master.columns\n",
    "# get today's datetime with a designated format\n",
    "today = datetime.now(pytz.timezone('US/Pacific')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# set the path for the dataframe copy\n",
    "base_copy =f'{download_path}/copy'\n",
    "\n",
    "# set the path to save the master pool copy\n",
    "copy_path_master_pool = f'{base_copy}/copy_master_pool'\n",
    "# set the path to save the manual copy\n",
    "copy_path_manual = f'{base_copy}/copy_manual'\n",
    "# set the path to save the echotik copy\n",
    "copy_path_echotik = f'{base_copy}/copy_echotik'\n",
    "\n",
    "# save the cleaned master DataFrame to a CSV file\n",
    "# the file name includes the current date and time\n",
    "master.to_csv(f'{copy_path_master_pool}/{today}_affiliate_masterpool.csv', index=False)\n",
    "\n",
    "# print a message indicating that the file has been saved successfully\n",
    "print(f\"File {today}_affiliate_masterpool.csv has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the master pool CSV file into a DataFrame\n",
    "# its only returns the 1st file within the master_pool folder\n",
    "# dtype=str ensures that all data is read to avoid datatype mismatches\n",
    "master = pd.read_csv(f'{master_pool_path}/{main_file_names[0]}', dtype=str)\n",
    "\n",
    "# optimized code for data cleaning\n",
    "master['tt_handles'] = master['tt_handles'] \\\n",
    "    .str.replace('\\n', '', regex=True) \\  # remove the new line characters\n",
    "    .str.replace('\\r', '', regex=True) \\  # remove the carriage return characters preventing overwritten contents\n",
    "    .str.replace('\\s+', ' ', regex=True) \\  # replace multiple whitespaces with a single whitespace \n",
    "    .str.replace('\\xa0+', ' ', regex=True) \\  # replace one or more non-breaking space characters with a white space, allowing us to split across lines if needed\n",
    "    .str.strip()  # remove leading and trailing whitespace\n",
    "\n",
    "\n",
    "# get all column names for the master dataframe\n",
    "master_column = master.columns\n",
    "# get today's datetime with a designated format\n",
    "today = datetime.now(pytz.timezone('US/Pacific')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# set the path for the dataframe copy\n",
    "base_copy =f'{download_path}/copy'\n",
    "\n",
    "# set the path to save the master pool copy\n",
    "copy_path_master_pool = f'{base_copy}/copy_master_pool'\n",
    "# set the path to save the manual copy\n",
    "copy_path_manual = f'{base_copy}/copy_manual'\n",
    "# set the path to save the echotik copy\n",
    "copy_path_echotik = f'{base_copy}/copy_echotik'\n",
    "\n",
    "# save the cleaned master DataFrame to a CSV file\n",
    "# the file name includes the current date and time\n",
    "master.to_csv(f'{copy_path_master_pool}/{today}_affiliate_masterpool.csv', index=False)\n",
    "\n",
    "# print a message indicating that the file has been saved successfully\n",
    "print(f\"File {today}_affiliate_masterpool.csv has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1717192169179,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "xoNcfV0mcl9B"
   },
   "outputs": [],
   "source": [
    "# Delete file function\n",
    "def delete_file(file_path_to_delete):\n",
    "    \"\"\"\n",
    "    deletes a certain file if it exists at the given file path.\n",
    "\n",
    "    Parameters:\n",
    "    file_path_to_delete (str): The path of the file to be deleted.\n",
    "\n",
    "    Returns:\n",
    "    Message indicates file was deleted or file path doesn't exist\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path_to_delete):\n",
    "        os.remove(file_path_to_delete)\n",
    "        print(f\"Delete: File {file_path_to_delete} has been deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File {file_path_to_delete} not found.\")\n",
    "\n",
    "# Update info function\n",
    "def update_info(df, col_name):\n",
    "    # if we have new value in columns, it will update to the new value\n",
    "    \"\"\"\n",
    "    Updates a specified column in the DataFrame with new values from associated columns and drop those columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data to update.\n",
    "    col_name (str): The column to update.\n",
    "\n",
    "    Returns:\n",
    "    a new column that contains rows from col_name_2 and col_name 1 into a single column but prioritize col_name_2\n",
    "    \"\"\"\n",
    "    df[col_name] = df.apply(lambda row: row[f'{col_name}_2'] if pd.notna(row[f'{col_name}_2'])\n",
    "                                                else row[f'{col_name}_1'], axis=1)\n",
    "    df.drop(columns=[f'{col_name}_1', f'{col_name}_2'], inplace=True)\n",
    "\n",
    "# The function to get unique category in uploading\n",
    "def unique_category (list1, list2):\n",
    "    # merge the lists and remove duplicate items by converting them to a set\n",
    "    \"\"\"\n",
    "    Merges two lists and returns a comma-separated string of unique elements.\n",
    "\n",
    "    Parameters:\n",
    "    list1 (list): The first list of elements.\n",
    "    list2 (list): The second list of elements.\n",
    "\n",
    "    Returns:\n",
    "    str: A comma-separated string of unique elements from both lists.\n",
    "    \"\"\"\n",
    "    unique_elements = set(list1) | set(list2) #|means union two sets and only take unique values (drop duplicates)\n",
    " \n",
    "    # convert the set back to a list (Optional: you can sort the list here with sorted(list(unique_elements)))\n",
    "    combined_list = list(set(unique_elements))\n",
    "\n",
    "    # convert the list of elements into a comma-separated string\n",
    "    # we use map(str, combined_list) to ensure all elements are strings, as join() requires string operands\n",
    "    result_string = \", \".join(map(str, combined_list))\n",
    "\n",
    "    return result_string\n",
    "\n",
    "# renew brief and thank you notes\n",
    "def renew_upload_by (unique_combinations, df_new, col_name):\n",
    "    \"\"\"\n",
    "    Updates a specified column in the df_new based on unique combinations from unique_combinations.\n",
    "\n",
    "    Parameters:\n",
    "    unique_combinations (DataFrame): DataFrame containing unique combinations to check against.\n",
    "    df_new (DataFrame): The DataFrame to update.\n",
    "    col_name (str): The name of the column to update.\n",
    "\n",
    "    Returns:\n",
    "    updated column in df_new with values from unique_combinations dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    for index, row in unique_combinations.iterrows():\n",
    "        df_new['tt_handles'] = df_new['tt_handles'].astype(str)\n",
    "        row['tt_handles'] = row['tt_handles'].astype(str)\n",
    "        mask = df_new['tt_handles'] == row['tt_handles']\n",
    "        df_new.loc[mask, col_name] = row[col_name]\n",
    "\n",
    "\n",
    "# def delete_creator(file_name, df1):\n",
    "#     # Load the excel file into a pandas DataFrame\n",
    "#     df = pd.read_excel(f'{delete_creator_path}/{file_name}', dtype = str)\n",
    "#     df2 = df.copy()\n",
    "\n",
    "#     df1[['tt_handles', 'email']] = df1[['tt_handles', 'email']].astype(str)\n",
    "#     df2[['tt_handles', 'email']] = df2[['tt_handles', 'email']].astype(str)\n",
    "\n",
    "#     handles_df1 = df1['tt_handles'].isin(df2['tt_handles']) & (df1['tt_handles'] != \"nan\")\n",
    "#     handles_df2 = df2['tt_handles'].isin(df1['tt_handles']) & (df2['tt_handles'] != \"nan\")\n",
    "#     email_df1 = df1['email'].isin(df2['email']) & (df1['email'] != \"nan\")\n",
    "#     email_df2 = df2['email'].isin(df1['email']) & (df2['email'] != \"nan\")\n",
    "\n",
    "#     df1_filtered = df1[~(handles_df1 | email_df1)]\n",
    "\n",
    "#     df2_filtered = df2[~(handles_df2 | email_df2)]\n",
    "\n",
    "#     df1_filtered = df1_filtered.replace('nan', '')\n",
    "#     df2_filtered = df2_filtered.replace('nan', '')\n",
    "#     return df1_filtered, df2_filtered\n",
    "\n",
    "# update the dataset with blacklisted users\n",
    "def black_list(file_name, df1):\n",
    "    \"\"\"\n",
    "    Marks entries in the dataframe as blacklisted based on matching tt_handles from an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    file_name (str): The name of the Excel file containing blacklisted tt_handles.\n",
    "    df1 (DataFrame): The dataframe to update with blacklist information.\n",
    "\n",
    "    Returns:\n",
    "    df1 (DataFrame): updated dataframe with blacklist information.\n",
    "    df3 (DataFrame): dataframe with tt_handles not found in the df1.\n",
    "    \"\"\"\n",
    "    # Load the excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(f'{delete_creator_path}/{file_name}', dtype = str)\n",
    "    df2 = df.copy()\n",
    "    # return the boolean and convert to integer \n",
    "    # df1['blacklist'] = df1['tt_handles'].isin(df2['tt_handles']).astype(int)\n",
    "    # df3 = df2[~(df2['tt_handles'].isin(df1['tt_handles']))]\n",
    "    for name in df2['tt_handles']:\n",
    "        df1.loc[df1['tt_handles'] == name, 'blacklist'] = 1\n",
    "    df3 = df2[~(df2['tt_handles'].isin(df1['tt_handles']))]\n",
    "    return df1, df3\n",
    "\n",
    "\n",
    "def clean_category_str(cat_str):\n",
    "    \"\"\"\n",
    "    Cleans a comma-separated category string by removing unwanted substrings like 'nan' and '0'.\n",
    "\n",
    "    Parameters:\n",
    "    cat_str (str): The category string to clean.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned category string.\n",
    "    \"\"\"\n",
    "    # Remove any 'nan' or '0' that are surrounded by commas and whitespace\n",
    "    # '\\b\\s*(nan|0)\\s*,\\s*': 'nan' or '0' at the beginning of a string surrounded by whitespace, \n",
    "    # followed by a comma and more whitespace\n",
    "    # '\\s*,\\s*\\b(nan|0)\\b': 'nan' or '0' at the end of a string surrounded by whitespace, \n",
    "    # preceded by a comma and more whitespace.\n",
    "    cleaned_str = re.sub(r'\\b\\s*(nan|0)\\s*,\\s*|\\s*,\\s*\\b(nan|0)\\b', '', cat_str)\n",
    "    # Remove any 'nan' or '0' if it is the only content of the string.\n",
    "    # '^\\s*': whitespace at the beginning of the string.\n",
    "    # (nan|0): 'nan' or '0'\n",
    "    # \\s*$: whitespace at the end of the string. \n",
    "    cleaned_str = re.sub(r'^\\s*(nan|0)\\s*$', '', cleaned_str)\n",
    "    # remove any leading and trailing whitespaces\n",
    "    cleaned_str = cleaned_str.strip(', ')\n",
    "    return cleaned_str\n",
    "\n",
    "\n",
    "def update_follower_tier(val):\n",
    "    \"\"\"\n",
    "    Determines the follower tier based on the number of followers.\n",
    "\n",
    "    Parameters:\n",
    "    val (int): The number of followers.\n",
    "\n",
    "    Returns:\n",
    "    str: The follower tier ('Macro', 'Micro', 'Nano'), or None if val is 0 or negative.\n",
    "    \"\"\"\n",
    "    if val >= 100:\n",
    "        return \"Macro\"\n",
    "    elif 100 > val >= 10:\n",
    "        return \"Micro\"\n",
    "    elif val > 0:\n",
    "        return \"Nano\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def update_so_num_contact(col_list, data):\n",
    "    \"\"\"\n",
    "    Updates the 'so_num_contact' column by summing values from specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    col_list (list): List of column names to sum.\n",
    "    data (DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: updated dataframe with the 'so_num_contact' column.\n",
    "    \"\"\"\n",
    "    # convert column names to strings and compile into regular expression object from re pattern\n",
    "    col_patterns = [re.compile(rf\"{str(col)}\") for col in col_list]\n",
    "\n",
    "    # find all columns in data that match the patterns\n",
    "    # use .search() method from the re object\n",
    "    columns_need_to_add = [col for col in data.columns if any(pat.search(col) for pat in col_patterns)]\n",
    "\n",
    "    # convert the matching columns to float and initialize 'so_num_contact' to 0\n",
    "    data[columns_need_to_add] = data[columns_need_to_add].astype(float)\n",
    "    data['so_num_contact'] = data[columns_need_to_add].sum(axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717192169179,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "KmIlCDkbkIzB"
   },
   "outputs": [],
   "source": [
    "# The function to data from echotik\n",
    "def echotik_clean_data(file_name):\n",
    "    \"\"\"\n",
    "    Cleans and processes data from Echotik files.\n",
    "\n",
    "    Parameter:\n",
    "    file_name (str): The name of the Excel file to be processed.\n",
    "\n",
    "    Returns:\n",
    "    df_output (DataFrame): A cleaned dataframe with relevant columns and formatted rows.\n",
    "    \"\"\"\n",
    "    # read file\n",
    "    df = pd.read_excel(f'{echotik_folder_path}/{file_name}', dtype=str)\n",
    "    \n",
    "    # save a copy of the file marked as today's version\n",
    "    df.to_excel(f'{copy_path_echotik}/{today}_{file_name}', index = False)\n",
    "    print(f\"Save: File {today}_{file_name} has been saved successfully.\")\n",
    "\n",
    "    # make a copy of the DataFrame for cleaning\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # reset the column names using the first row and drop the first row\n",
    "    df_clean.columns = df_clean.loc[0] # use the 1st row as the column row in the excel sheet\n",
    "    df_clean = df_clean.drop(df_clean.index[0]) # drop the 1st row as we use it as columns\n",
    "    df_clean.columns = [col.lower() for col in df_clean.columns] # ensure columns are all lower cases\n",
    "    # print(df_clean.columns)\n",
    "    \n",
    "    # identify the main category column\n",
    "    if \"influencer category\" in df_clean.columns:\n",
    "        main_filtered_elements = [\"influencer category\"]\n",
    "        df_clean[main_filtered_elements[0]] = '' # set all values in the influencer category column as ''\n",
    "    else:\n",
    "        main_filtered_elements = [element for element in df_clean.columns if \"product\" in element or \"category\" in element]\n",
    "    \n",
    "    # splits the file_name string by spaces and takes the first part (before the first space) replace any & with ', '\n",
    "    # extract detailed category value from file name\n",
    "    detailed_category_value = file_name.split(' ')[0].lower().replace('&', ', ') \n",
    "    df_clean['detailed_category'] = detailed_category_value\n",
    "    # print(main_filtered_elements)\n",
    "    # print(df_clean.columns)\n",
    "    \n",
    "    # get useful columns\n",
    "    df_clean = df_clean[['unique id', 'contact email', 'followers', \n",
    "                         main_filtered_elements[0], 'detailed_category']]\n",
    "    df_clean.columns = ['tt_handles','email', 'follower_num', 'main_category', 'detailed_category']\n",
    "    df_clean['main_category'] = df_clean['main_category'].apply(lambda x: str(x).lower().replace(' & ', ', '))\n",
    "    \n",
    "    # drop duplicate (what if the creater sells multiple categories and we lost the info)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['tt_handles'])\n",
    "    # convert follower_num to float and normalize units\n",
    "    df_clean['follower_num'] = df_clean['follower_num'].apply(lambda x: float(str(x).replace('K', '')) if 'K' in str(x)\n",
    "                                                                                else (float(str(x).replace('M', '')) * 1000 if 'M' in str(x)\n",
    "                                                                                        else float(x)/1000))\n",
    "    # get correct format for tt_handles\n",
    "    df_clean['tt_handles'] = df_clean['tt_handles'].apply(lambda x: str(x).replace('@', ''))\n",
    "    # drop row without email\n",
    "    df_clean = df_clean.dropna(subset=['email']).copy()\n",
    "    # replace nans whith empty string\n",
    "    df_clean = df_clean.replace('nan', '')\n",
    "    # create a new column 'upload_date' with current date\n",
    "    df_clean['upload_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    # create an output dataframe with relevant columns \n",
    "    df_output  = df_clean[['tt_handles', 'email', 'follower_num', \n",
    "                           'main_category', 'detailed_category', 'upload_date']].copy()\n",
    "    # check column names are correct\n",
    "    print(df_clean['tt_handles'].iloc[0])\n",
    "    # check the shape of the dataset\n",
    "    print(df_output.shape)\n",
    "    # print('__________')\n",
    "    return df_output\n",
    "\n",
    "def manual_clean_data(file_name):\n",
    "    \"\"\"\n",
    "    Cleans and processes data from manual folder files.\n",
    "\n",
    "    Parameters:\n",
    "    file_name (str): The name of the Excel file to be processed.\n",
    "\n",
    "    Returns:\n",
    "    df_valid (DataFrame): A cleaned dataframe with relevant columns and formatted data, or an empty dataframe if errors are found.\n",
    "    \"\"\"\n",
    "    # Load the excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(f'{manual_folder_path}/{file_name}', dtype=str)\n",
    "    \n",
    "    # save it to excel file\n",
    "    df.to_excel(f'{copy_path_manual}/{today}_{file}', index = False)\n",
    "    print(f\"Save: File {today}_{file} has been saved successfully.\")\n",
    "    \n",
    "    # get the 1st part of the file name splited by .\n",
    "    partial_name = file_name.split(\".\")[0]\n",
    "    df_column = df.columns\n",
    "    columns_not_in_masterpool = df.columns[~df.columns.isin(master.columns)]\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # initialize an empty dataframe\n",
    "    df_valid = pd.DataFrame()\n",
    "    \n",
    "    # if the dataframe has 0 rows\n",
    "    if df_clean.shape[0] == 0:\n",
    "        df_clean.to_excel(f'{error_check_path}/{today}_{partial_name}_notthandles.xlsx', index = False)\n",
    "        print(f'Error empty table: {error_check_path}/{partial_name}_empty.xlsx has been created')\n",
    "        return df_valid\n",
    "    \n",
    "    #  check if 'tt_handles' column is present\n",
    "    if 'tt_handles' not in df_column:\n",
    "        df_invalid = df_clean.copy()\n",
    "        df_invalid.to_excel(f'{error_check_path}/{today}_{partial_name}_notthandles.xlsx', index = False)\n",
    "        print(f'Error tt_handles: {error_check_path}/{partial_name}_notthandles.xlsx has been created')\n",
    "        return df_valid\n",
    "    \n",
    "    # check for columns not in the master pool\n",
    "    if len(columns_not_in_masterpool) > 0:\n",
    "        # print('wrong columns：', columns_not_in_masterpool)\n",
    "        wrong_column = ', '.join(columns_not_in_masterpool)\n",
    "        df_invalid = df_clean.copy()\n",
    "        df_invalid.to_excel(f'{error_check_path}/{today}_{partial_name}_wrongcolumn_{wrong_column}.xlsx', index = False)\n",
    "        print(f'Error wrong_columns: {error_check_path}/{partial_name}_wrongcolumn_{wrong_column}.xlsx has been created')\n",
    "        return df_valid\n",
    "    \n",
    "    # clean and format 'first_name' column if present\n",
    "    if 'first_name' in df_column:\n",
    "        # Check if first_name has no spaces and starts with an uppercase followed by lowercase letters\n",
    "        df_clean['first_name'] = df_clean['first_name'].apply(lambda x: ''.join(str(x).split()) if x != '' else x)\n",
    "        # remove all whitespaces characters like tab, whitespaces, newlines\n",
    "        df_clean['first_name'] = df_clean['first_name'].replace(\"\\s+\", \"\", regex=True)\n",
    "        # capitalizing the first letter of each word and converting all other letters to lowercase.\n",
    "        df_clean['first_name'] = df_clean['first_name'].str.title()\n",
    "    \n",
    "    # convert 'main_category' column to lowercase if present\n",
    "    if 'main_category' in df_column:\n",
    "        # Ensure video_category is in lowercase\n",
    "        df_clean['main_category'] = df_clean['main_category'].str.lower()\n",
    "    \n",
    "    # validate and format 'upload_date' column if present\n",
    "    if 'upload_date' in df_column:\n",
    "        df_invalid = df_clean[df_clean['upload_date'].isna()]\n",
    "        # print(df_invalid.head())\n",
    "        if df_invalid.shape[0] > 0:\n",
    "            df_clean.to_excel(f'{error_check_path}/{today}_{partial_name}_no_uploaddate.xlsx', index = False)\n",
    "            print(f'Error upload_date: {error_check_path}/{partial_name}_no_uploaddate has been created')\n",
    "            return df_valid\n",
    "        # try and except blocks check upload date is present or not\n",
    "        try:\n",
    "            df_clean['upload_date'] = pd.to_datetime(df_clean['upload_date']).dt.date\n",
    "            # df_clean['upload_date'] = df_clean['upload_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        except ValueError:\n",
    "            df_clean.to_excel(f'{error_check_path}/{today}_{partial_name}_uploaddate_wrongformat.xlsx', index = False)\n",
    "            print(f'Error upload_date: {error_check_path}/{partial_name}_uploaddate_wrongformat has been created')\n",
    "            return df_valid\n",
    "    \n",
    "    # validate email format if 'email' column is present\n",
    "    if 'email' in df_column:\n",
    "        # Regex pattern to check for a valid email format\n",
    "        # ^means at the beginning, +means at least once, $means at the end\n",
    "        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        # Apply a lambda function to check if each email matches the pattern\n",
    "        df_clean['email'] = df_clean['email'].apply(lambda x: ''.join(str(x).split()) if pd.notna(x) else x)\n",
    "        # remove any whitespaces characters\n",
    "        df_clean['email'] = df_clean['email'].replace(\"\\s+\", \"\", regex=True)\n",
    "        # check if a row in email column has nan or ''\n",
    "        df_clean['valid_email'] = df_clean['email'].apply(lambda x: bool(re.match(pattern, str(x))) if pd.notna(x) and x != '' else False)\n",
    "\n",
    "        # filter out invalid emails\n",
    "        df_clean = df_clean[df_clean['valid_email']].copy()\n",
    "        df_invalid = df_clean[~(df_clean['valid_email'])].copy()\n",
    "        if df_invalid.shape[0] > 0:\n",
    "            df_invalid.to_excel(f'{error_check_path}/{today}_{partial_name}_error_email.xlsx', index = False)\n",
    "            print(f'Error email: {error_check_path}/{partial_name}_error_email.xlsx has been created')\n",
    "    df_clean = df.replace('nan', '')\n",
    "    df_valid = df_clean[df_column].copy()\n",
    "    # deleted merged manual table\n",
    "    # file_path_to_delete = f'{manual_folder_path}/{file_name}'\n",
    "    # delete_file(file_path_to_delete)\n",
    "\n",
    "    return df_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1717192169179,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "-q0DzNbHqBxJ"
   },
   "outputs": [],
   "source": [
    "# The function to update clean data to the original master pool\n",
    "def combine_category(df1, df2):\n",
    "    \"\"\"\n",
    "    Combines the categories from two dataframes based on 'tt_handles'.\n",
    "    \n",
    "    Args:\n",
    "    df1 (DataFrame): First dataframe containing TikTok handles and categories.\n",
    "    df2 (DataFrame): Second dataframe containing TikTok handles and categories.\n",
    "\n",
    "    Returns:\n",
    "    merged_df(DataFrame): A merged dataframe with combined categories and other relevant columns.\n",
    "    \"\"\"\n",
    "    # combining the categories from both dataframes using outer join\n",
    "    merged_df = pd.merge(df1, df2, on=['tt_handles'], how='outer', suffixes=('_1', '_2'))\n",
    "    merged_df_column = merged_df.columns\n",
    "    # print(merged_df_column)\n",
    "    # print('main_category_1' in merged_df_column)\n",
    "    # print('upload_date_1' in merged_df_column)\n",
    "    \n",
    "    # check and process 'main_category' columns\n",
    "    if  'main_category_1' in merged_df_column:\n",
    "        # convert 'main_category_1' and 'main_category_2' columns to string type\n",
    "        merged_df[['main_category_1', 'main_category_2']] = merged_df[['main_category_1', 'main_category_2']].astype(str)\n",
    "        # combine 'main_category' columns and apply unique_category function and split values based on comma or white spaces\n",
    "        merged_df['main_category'] = merged_df.apply(\n",
    "            lambda row: unique_category(\n",
    "                re.split(',\\s*', row['main_category_1']),\n",
    "                re.split(',\\s*', row['main_category_2'])\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        # drop the original 'main_category' columns\n",
    "        merged_df.drop(columns=['main_category_1', 'main_category_2'], inplace=True)\n",
    "        # clean the combined 'main_category' string\n",
    "        merged_df['main_category'] = merged_df['main_category'].apply(lambda x: clean_category_str(x))\n",
    "        \n",
    "        # merged_df['main_category'] = merged_df['main_category'].str.replace(r'\\b(nan|0)\\b\\s*(,\\s*)?', '', regex=True)\n",
    "        # merged_df['main_category'] = merged_df['main_category'].str.replace(r',\\s*,', ', ', regex=True)\n",
    "        # merged_df['main_category'] = merged_df['main_category'].str.strip(', ').str.rstrip(',')\n",
    "        \n",
    "    # check and process 'detailed_category' columns\n",
    "    if  'detailed_category_1' in merged_df_column:\n",
    "        # convert 'detailed_category_1' and 'detailed_category_2' columns to string type\n",
    "        merged_df[['detailed_category_1', 'detailed_category_2']] = merged_df[['detailed_category_1', 'detailed_category_2']].astype(str)\n",
    "        # combine 'detailed_category' columns and apply unique_category function\n",
    "        merged_df['detailed_category'] = merged_df.apply(\n",
    "            lambda row: unique_category(\n",
    "                re.split(',\\s*', row['detailed_category_1']),\n",
    "                re.split(',\\s*', row['detailed_category_2'])\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        # drop the original 'detailed_category' columns\n",
    "        merged_df.drop(columns=['detailed_category_1', 'detailed_category_2'], inplace=True)\n",
    "        # clean the combined 'detailed_category' string\n",
    "        merged_df['detailed_category'] = merged_df['detailed_category'].apply(lambda x: clean_category_str(x))\n",
    "    # check and process 'upload_date' columns\n",
    "    if 'upload_date_1' in merged_df_column:\n",
    "        # use 'upload_date_1' if available, otherwise use 'upload_date_2'\n",
    "        merged_df['upload_date'] = merged_df.apply(lambda row: row['upload_date_1'] if pd.notna(row['upload_date_1']) else row['upload_date_2'], axis=1)\n",
    "        # drop the original 'upload_date' columns\n",
    "        merged_df.drop(columns=['upload_date_1', 'upload_date_2'], inplace=True)\n",
    "    #  Process so_num_onboardings, so_num_newrelease, so_num_discount, so_num_tap, so_num_tap_comfirm for summation\n",
    "    for col_name in df2.columns:\n",
    "        if 'so_num_onboardings' in col_name or 'so_num_newrelease' in col_name or 'so_num_discount' in col_name or 'so_num_tap' in col_name or 'so_num_tap_comfirm' in col_name:\n",
    "            # concatenate corresponding columns and drop the originals\n",
    "            merged_df[col_name] = merged_df[f'{col_name}_1'].astype(str) + merged_df[f'{col_name}_2'].astype(str)\n",
    "            merged_df.drop(columns=[f'{col_name}_1', f'{col_name}_2'], inplace=True)\n",
    "            # print(col_name)\n",
    "        # update remaining columns, excluding 'tt_handles', 'main_category', 'upload_date', and 'detailed_category'\n",
    "        elif col_name != 'tt_handles' and col_name != 'main_category' and col_name != 'upload_date' and col_name != 'detailed_category':\n",
    "            update_info(merged_df, col_name)\n",
    "    \n",
    "    # replace 'nan' with empty strings\n",
    "    merged_df = merged_df.replace('nan', '')\n",
    "    # drop duplicate rows based on 'tt_handles'\n",
    "    merged_df = merged_df.drop_duplicates(subset=['tt_handles'], keep='first')\n",
    "    print(merged_df.shape)\n",
    "    print('------------------------')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14995,
     "status": "ok",
     "timestamp": 1717192184158,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "_kdBIfw1qpvO",
    "outputId": "5b2b2c64-8254-4abe-f7a1-277f7dcbfec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (60325, 125)\n",
      "Fhaye_scouting_DolceJbNhie_20240520.xlsx\n",
      "Save: File 2024-05-31 14:49:25_Fhaye_scouting_DolceJbNhie_20240520.xlsx has been saved successfully.\n",
      "(60323, 125)\n",
      "------------------------\n",
      "emails_20240531.xlsx\n",
      "Save: File 2024-05-31 14:49:25_emails_20240531.xlsx has been saved successfully.\n",
      "(60323, 125)\n",
      "------------------------\n",
      "Elaiza_updaterate_20240531.xlsx\n",
      "Save: File 2024-05-31 14:49:25_Elaiza_updaterate_20240531.xlsx has been saved successfully.\n",
      "Error tt_handles: /content/drive/MyDrive/so_affiliate/master_pool/upload_error_check/Elaiza_updaterate_20240531_notthandles.xlsx has been created\n",
      "------------------------\n",
      "After Manual data shape: (60323, 125)\n",
      "Wrong tt_handles in blacklist: (15, 1)\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# create a copy of master dataframe\n",
    "df1 = master.copy()\n",
    "print(f'Original data shape: {df1.shape}')\n",
    "\n",
    "# apply the previously defined echotik_clean_data function clean datasets in echotik folder\n",
    "if len(echotik_file_names) > 0:\n",
    "    for file in echotik_file_names:\n",
    "        print(file)\n",
    "        # clean each data form echotik\n",
    "        df2 = echotik_clean_data(file)\n",
    "\n",
    "        # df2.to_excel(f'{copy_path_echotik}/{today}_{file}', index = False)\n",
    "        # print(f\"Save: File {today}_{file} has been saved successfully.\")\n",
    "\n",
    "        # delete merged echotik table\n",
    "        # file_path_to_delete = f'{echotik_folder_path}/{file}'\n",
    "        # delete_file(file_path_to_delete)\n",
    "\n",
    "        # merge these clean data into master pool\n",
    "        df_result = combine_category(df1, df2)\n",
    "\n",
    "        df1 = df_result.copy()\n",
    "    print(f'After enchotik data shape: {df1.shape}')\n",
    "\n",
    "# apply the previously defined manual_clean_data function clean datasets in manual upload folder\n",
    "if len(manual_file_names) > 0:\n",
    "    # clean data in manual upload\n",
    "    for file in manual_file_names:\n",
    "        print(file)\n",
    "        df2 = manual_clean_data(file)\n",
    "\n",
    "\n",
    "        # deleted merged manual table\n",
    "        # file_path_to_delete = f'{manual_folder_path}/{file}'\n",
    "        # delete_file(file_path_to_delete)\n",
    "\n",
    "        if df2.shape[0] > 0:\n",
    "            # merge manual table master pool\n",
    "            df_result = combine_category(df1, df2)\n",
    "            df1 = df_result.copy()\n",
    "        else:\n",
    "            print('------------------------')\n",
    "    print(f'After Manual data shape: {df1.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# apply the previously defined black_list function clean datasets in blacklist folder\n",
    "df_result, blacklist_creator_table = black_list(deletecreator_file_names[0], df1)\n",
    "# save the table to the folder path\n",
    "blacklist_creator_table.to_excel(f'{delete_creator_path}/{deletecreator_file_names[0]}',index=False)\n",
    "print(f'Wrong tt_handles in blacklist: {blacklist_creator_table.shape}')\n",
    "print('*************************')\n",
    "df1 = df_result.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1717192184158,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "ZkRtEGaIUyA8"
   },
   "outputs": [],
   "source": [
    "# define today's date in 'US/Pacific' timezone\n",
    "today = datetime.now(pytz.timezone('US/Pacific')).strftime('%Y-%m-%d')\n",
    "def parse_date_with_multiple_formats(date_str):\n",
    "    \"\"\"\n",
    "    Parses a date string using multiple formats and returns a date object.\n",
    "    \n",
    "    If the date string does not match any of the provided formats, today's date\n",
    "    in 'US/Pacific' timezone is returned.\n",
    "\n",
    "    Parameters:\n",
    "    date_str (str): The date string to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    datetime.date object: Parsed date object or today's date if parsing fails.\n",
    "    \"\"\"\n",
    "    # List of date formats to try\n",
    "    formats = ['%Y-%m-%d','%Y-%m-%d %H:%M:%S']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt).date()\n",
    "        except ValueError:\n",
    "            # print(date_str)\n",
    "            # if parsing fails, continue to the next format\n",
    "            continue\n",
    "    # if all formats fail, return today's date\n",
    "    return today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12468,
     "status": "ok",
     "timestamp": 1717192196609,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "WOQkFG-S_1pw",
    "outputId": "e4b086b7-2301-44c0-9939-3a4dcbd60cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/so_affiliate/master_pool/master_pool_view/affiliate_masterpool.csv has been updated\n"
     ]
    }
   ],
   "source": [
    "# create a copy of df1 with the columns specified in master_column\n",
    "final_df = df1[master_column].copy()\n",
    "# convert 'follower_num' column to numeric, coercing errors to NaN\n",
    "final_df['follower_num'] = pd.to_numeric(final_df['follower_num'], errors='coerce')\n",
    "# update 'follower_tier' column based on the numeric values in 'follower_num' using update_follower_tier function\n",
    "final_df['follower_tier'] = final_df['follower_num'].apply(update_follower_tier)\n",
    "# list of columns to update for so_num_contact\n",
    "col_list = ['so_num_onboardings', 'so_num_newrelease', 'so_num_discount']\n",
    "# update specific columns in final_df using the update_so_num_contact function\n",
    "final_df = update_so_num_contact(col_list, final_df)\n",
    "# replace all nan in the dataframe with empty string\n",
    "final_df = final_df.replace('nan', '')\n",
    "# set 'upload_date' to today's date if it is null\n",
    "final_df.loc[final_df['upload_date'].isnull(), 'upload_date'] = today\n",
    "# parse 'upload_date' column using the parse_date_with_multiple_formats function\n",
    "final_df['upload_date'] = final_df['upload_date'].apply(parse_date_with_multiple_formats)\n",
    "# convert 'upload_date' column to string format\n",
    "final_df['upload_date'] = final_df['upload_date'].astype(str)\n",
    "\n",
    "# export the final view\n",
    "final_df.to_csv(f'{master_pool_path}/affiliate_masterpool.csv', index = False)\n",
    "print(f\"{master_pool_path}/affiliate_masterpool.csv has been updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1717192196609,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "27sgl2Oy_91D",
    "outputId": "50768146-7005-4b5c-b6cd-770b594d2111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60325, 125)\n",
      "(60323, 125)\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "print(master.shape)\n",
    "print(final_df.shape)\n",
    "# number of rows difference between master df and final df\n",
    "print(final_df.shape[0] - master.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17851,
     "status": "ok",
     "timestamp": 1717192214458,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "eHg141kRACMn",
    "outputId": "1407b548-c10b-408f-f609-6703da4df7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/so_affiliate/master_pool/clean_view/clean_view.xlsx has been updated\n"
     ]
    }
   ],
   "source": [
    "# generate a clean version of the processed dataframe\n",
    "clean_view = final_df[['tt_handles', 'first_name', 'email', 'main_category', 'detailed_category',\n",
    "                 'gmv', 'avg_commission_rate', 'tts_commerce', 'follower_num', 'blacklist', 'avoid_contact', 'upload_date']].copy()\n",
    "clean_view.to_excel(f'{clean_view_path}/clean_view.xlsx', index = False)\n",
    "print(f\"{clean_view_path}/clean_view.xlsx has been updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57126,
     "status": "ok",
     "timestamp": 1717192271563,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "FqQndVO8BwfR",
    "outputId": "7d42aa4a-d041-4992-f38a-f5284c77e473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jenny_view:  ['jenny_view.xlsx']\n",
      "/content/drive/MyDrive/so_affiliate/master_pool/jenny_view/jenny_view.xlsx has been updated\n"
     ]
    }
   ],
   "source": [
    "jenny_view_path = f'{base_path}/jenny_view'\n",
    "jennyview_file_names = [f for f in os.listdir(jenny_view_path) if os.path.isfile(os.path.join(jenny_view_path, f))]\n",
    "print('jenny_view: ', jennyview_file_names)\n",
    "\n",
    "jenny_view = final_df[['tt_handles', 'first_name', 'email', 'main_category', 'detailed_category',\n",
    "                 'avg_vv', 'avg_engagement', 'gmv', 'avg_commission_rate', 'units_sold',\n",
    "                 'ave_sv_vv', 'avg_sv_engagement',\n",
    "                 'tts_commerce', 'gender','follower_num', 'tts_rate', 'brand_collab', 'prod_price',\n",
    "                 'so_num_collab', 'so_num_collab_milk','so_num_collab_babor', 'so_num_collab_nhie', 'so_num_collab_byrd',\n",
    "                 'so_num_collab_pacas', 'so_num_collab_ptr','so_num_collab_juice_beauty', 'so_num_collab_dolce_glow',\n",
    "                 'follower_male_percent', 'follower_female_percent',\n",
    "                 'follower_age18_24_percent','follower_age25_34_percent', 'follower_age35_44_percent', 'follower_age45_54_percent', 'follower_age_over55_percent',\n",
    "                 'blacklist','avoid_contact', 'upload_date']].copy()\n",
    "jenny_view.to_excel(f'{jenny_view_path}/jenny_view.xlsx', index = False)\n",
    "print(f\"{jenny_view_path}/jenny_view.xlsx has been updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717192271563,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "w3pSs-1rnNiN",
    "outputId": "356ac064-8f30-4b81-a234-6271c0a4738a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-37e16529-67d8-4939-9561-82323cef47b6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tt_handles</th>\n",
       "      <th>follower_tier</th>\n",
       "      <th>social_media</th>\n",
       "      <th>gender</th>\n",
       "      <th>base_in</th>\n",
       "      <th>age_des</th>\n",
       "      <th>detailed_category</th>\n",
       "      <th>video_num</th>\n",
       "      <th>avg_vv</th>\n",
       "      <th>avg_like</th>\n",
       "      <th>...</th>\n",
       "      <th>so_num_collab_nhie</th>\n",
       "      <th>so_num_collab_byrd</th>\n",
       "      <th>so_num_collab_pacas</th>\n",
       "      <th>so_num_collab_ptr</th>\n",
       "      <th>so_num_collab_juice_beauty</th>\n",
       "      <th>so_num_collab_dolce_glow</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>so_num_collab_tower28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 125 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e16529-67d8-4939-9561-82323cef47b6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-37e16529-67d8-4939-9561-82323cef47b6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-37e16529-67d8-4939-9561-82323cef47b6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tt_handles, follower_tier, social_media, gender, base_in, age_des, detailed_category, video_num, avg_vv, avg_like, avg_comment, avg_share, avg_save, avg_engagement, avg_duration, top1_vv, top1_like, top3_videolink, tts_rate, so_num_collab, so_tik_star, so_num_contact, top1_location, top1_location_percentage, top2_location, top2_location_percentage, top3_location, top3_location_percentage, so_num_onboardings_milk, so_num_newrelease_milk, so_num_discount_milk, so_num_tap_milk, so_num_tap_comfirm_milk, so_num_onboardings_babor, so_num_newrelease_babor, so_num_discount_babor, so_num_tap_babor, so_num_tap_comfirm_babor, so_num_onboardings_byrd, so_num_newrelease_byrd, so_num_discount_byrd, so_num_tap_byrd, so_num_tap_comfirm_byrd, so_num_onboardings_nhie, so_num_newrelease_nhie, so_num_discount_nhie, so_num_tap_nhie, so_num_tap_comfirm_nhie, so_num_onboardings_pacas, so_num_newrelease_pacas, so_num_discount_pacas, so_num_tap_pacas, so_num_tap_comfirm_pacas, so_num_onboardings_ptr, so_num_newrelease_ptr, so_num_discount_ptr, so_num_tap_ptr, so_num_tap_comfirm_ptr, so_num_onboardings_juicy_beauty, so_num_newrelease_juice_beauty, so_num_discount_juice_beauty, so_num_tap_juice_beauty, so_num_tap_comfirm_juice_beauty, blacklist, first_name, so_num_onboardings_juice_beauty, so_num_onboardings_dolce_glow, so_num_newrelease_dolce_glow, so_num_discount_dolce_glow, so_num_tap_dolce_glow, so_num_tap_comfirm_dolce_glow, uid, so_num_onboardings_tower28, so_num_newrelease_tower28, so_num_discount_tower28, so_num_tap_tower28, so_num_tap_comfirm_tower28, avoid_contact, main_category, follower_num, gmv, units_sold, gpm, gmv_per_buyer, avg_commission_rate, num_product, num_brand_collab, brand_collab, prod_price, video_gpm, sv_num, ave_sv_vv, avg_sv_engagement, avg_sv_like, avg_sv_comment, avg_sv_share, live_gpm, live_num, ave_live_viewer, avg_live_engagement, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 125 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['upload_date'].isnull()] # ensure the dataset contains no null or nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1717192271563,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "Zff2E5OO6MnO",
    "outputId": "a3a0ce11-8a2b-4090-fca3-59bfac473722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tt_handles', 'follower_tier', 'social_media', 'gender', 'base_in',\n",
      "       'age_des', 'detailed_category', 'video_num', 'avg_vv', 'avg_like',\n",
      "       'avg_comment', 'avg_share', 'avg_save', 'avg_engagement',\n",
      "       'avg_duration', 'top1_vv', 'top1_like', 'top3_videolink', 'tts_rate',\n",
      "       'so_num_collab', 'so_tik_star', 'so_num_contact', 'top1_location',\n",
      "       'top1_location_percentage', 'top2_location', 'top2_location_percentage',\n",
      "       'top3_location', 'top3_location_percentage', 'so_num_onboardings_milk',\n",
      "       'so_num_newrelease_milk', 'so_num_discount_milk', 'so_num_tap_milk',\n",
      "       'so_num_tap_comfirm_milk', 'so_num_onboardings_babor',\n",
      "       'so_num_newrelease_babor', 'so_num_discount_babor', 'so_num_tap_babor',\n",
      "       'so_num_tap_comfirm_babor', 'so_num_onboardings_byrd',\n",
      "       'so_num_newrelease_byrd', 'so_num_discount_byrd', 'so_num_tap_byrd',\n",
      "       'so_num_tap_comfirm_byrd', 'so_num_onboardings_nhie',\n",
      "       'so_num_newrelease_nhie', 'so_num_discount_nhie', 'so_num_tap_nhie',\n",
      "       'so_num_tap_comfirm_nhie', 'so_num_onboardings_pacas',\n",
      "       'so_num_newrelease_pacas', 'so_num_discount_pacas', 'so_num_tap_pacas',\n",
      "       'so_num_tap_comfirm_pacas', 'so_num_onboardings_ptr',\n",
      "       'so_num_newrelease_ptr', 'so_num_discount_ptr', 'so_num_tap_ptr',\n",
      "       'so_num_tap_comfirm_ptr', 'so_num_onboardings_juicy_beauty',\n",
      "       'so_num_newrelease_juice_beauty', 'so_num_discount_juice_beauty',\n",
      "       'so_num_tap_juice_beauty', 'so_num_tap_comfirm_juice_beauty',\n",
      "       'blacklist', 'first_name', 'so_num_onboardings_juice_beauty',\n",
      "       'so_num_onboardings_dolce_glow', 'so_num_newrelease_dolce_glow',\n",
      "       'so_num_discount_dolce_glow', 'so_num_tap_dolce_glow',\n",
      "       'so_num_tap_comfirm_dolce_glow', 'uid', 'so_num_onboardings_tower28',\n",
      "       'so_num_newrelease_tower28', 'so_num_discount_tower28',\n",
      "       'so_num_tap_tower28', 'so_num_tap_comfirm_tower28', 'avoid_contact',\n",
      "       'main_category', 'follower_num', 'gmv', 'units_sold', 'gpm',\n",
      "       'gmv_per_buyer', 'avg_commission_rate', 'num_product',\n",
      "       'num_brand_collab', 'brand_collab', 'prod_price', 'video_gpm', 'sv_num',\n",
      "       'ave_sv_vv', 'avg_sv_engagement', 'avg_sv_like', 'avg_sv_comment',\n",
      "       'avg_sv_share', 'live_gpm', 'live_num', 'ave_live_viewer',\n",
      "       'avg_live_engagement', 'avg_live_like', 'avg_live_comment',\n",
      "       'avg_live_share', 'follower_male_percent', 'follower_female_percent',\n",
      "       'follower_age18_24_percent', 'follower_age25_34_percent',\n",
      "       'follower_age35_44_percent', 'follower_age45_54_percent',\n",
      "       'follower_age_over55_percent', 'tts_commerce', 'cid', 'email',\n",
      "       'so_num_collab_milk', 'so_num_collab_babor', 'so_num_collab_nhie',\n",
      "       'so_num_collab_byrd', 'so_num_collab_pacas', 'so_num_collab_ptr',\n",
      "       'so_num_collab_juice_beauty', 'so_num_collab_dolce_glow', 'phone',\n",
      "       'address', 'upload_date', 'so_num_collab_tower28'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_seq_items', None) #display all elements in arrays, lists and so on.\n",
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1717192271565,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "xIPonAXOinYI",
    "outputId": "c7e3fc1d-9e6a-44bf-914a-a34436489991"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30065, 125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[~final_df['email'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1717192271565,
     "user": {
      "displayName": "tts_ops",
      "userId": "06153775275377967593"
     },
     "user_tz": 420
    },
    "id": "D9qsjZUWoKj7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM1pyinAgX7l3GsCef8ro0J",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
